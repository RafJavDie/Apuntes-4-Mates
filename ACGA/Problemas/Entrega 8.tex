\documentclass[twoside]{article}
\usepackage{../../estilo-ejercicios}
%\DeclareMathOperator{\Ima}{Im}

%--------------------------------------------------------
\begin{document}

\title{Algebra Conmutativa y Geometría Algebraica}
\author{Javier Aguilar Martín, Rafael González López, Diego Pedraza López}
\maketitle

\begin{ejercicio}{1}\mbox{}
\begin{itemize}
	\item Sea $A \in \mathcal{M}_{n+1}(k)$ una matriz $(n+1)\times(n+1)$ invertible con coeficientes en $k$. Probar que la aplicación $φ : \PP_k^n \to \PP_k^n$ dada por $φ(x_0:x_1:\dots:x_n) = (y_0:y_1:\dots:y_n)$ con
	\[ \begin{pmatrix}y_0\\y_1\\\vdots\\y_n\end{pmatrix} = A\begin{pmatrix}x_0\\x_1\\\vdots\\x_n\end{pmatrix}\]
	es un automorfismo de $\PP_k^n$. Diremos que $φ$ viene dado por la multiplicación por la matriz $A$.

	\item Probar que para todos $a,b\in\PP_k^1$ existe un automorfismo $φ : \PP_k^1 \to \PP_k^1$ tal que $φ(a)=b$.

	\item Probar que todo automorfismo de $\PP_k^1$ viene dado por la multiplicación por una matriz $A \in \mathcal{M}_2(k)$ (Ayuda: Considerar primero el caso en el que el punto del infinito es fijo, y usar el ejercicio 5 de la relación 6. Reducir el caso general a éste usando el apartado anterior).
\end{itemize}
\end{ejercicio}
\begin{solucion}\mbox{}
\begin{itemize}
\item Desarrollando la multiplicación por la matriz $A$, podemos escribir la aplicación $φ$ como:
\[ \begin{pmatrix}y_0\\y_1\\\vdots\\y_n\end{pmatrix} = \begin{pmatrix}\sum_{j=0}^n a_{0j}x_j\\\sum_{j=0}^n a_{1j}x_j\\\vdots\\\sum_{j=0}^n a_{nj}x_i\end{pmatrix} \]
donde $A = (a_{ij})_{i,j=0,\dots,n}$. Cada componente es un polinomio homogéneo de mismo grado. Además no se anulan simultáneamente en ningún punto de $\PP_k^n$, pues sabemos que el núcleo de una matriz invertible es $\{0\}$, que no está presente en $\PP_k^n$. Por lo tanto, $φ$ es un morfismo. Así mismo, la multiplicación de $A^{-1}$ define también un morfismo $ψ : \PP_k^n \to \PP_k^n$, pues $A^{-1}$ también es invertible. Finalmente, tenemos que $φ \circ ψ = ψ \circ φ = id$, pues $A^{-1}\cdot A = A \cdot A^{-1} = I$ y la multiplicación por $I$ describe el morfismo identidad.

\item Sea $a=(a_0:a_1) \in \PP_k^1$ y $b=(b_0:b_1) \in \PP_k^1$. Separemos los casos:
\begin{itemize}
	\item Si $a=(1:0)$. Entonces:
	\begin{itemize}
		\item Si $b = (1:b_1)$, definimos $φ$ por la multiplicación de la matriz:
		\[ A = \begin{pmatrix}1 & 0\\b_1 & 1\end{pmatrix} \]
		\item Si $b = (0:1)$, definimos $φ$ por la multiplicación de la matriz:
		\[ A = \begin{pmatrix}0 & 1\\1 & 0\end{pmatrix} \]
	\end{itemize}
	\item Si $a=(1:a_1)$ con $a_1 \neq 0$. Entonces:
	\begin{itemize}
		\item Si $b = (1:0)$, definimos $φ$ por la multiplicación de la matriz:
		\[ A = \begin{pmatrix}1 & 0\\1 & -1/a_1\end{pmatrix} \]
		\item Si $b = (1:b_1)$ con $b_1 \neq 0$, definimos $φ$ por la multiplicación de la matriz:
		\[ A = \begin{pmatrix}1 & 0\\0 & b_1/a_1\end{pmatrix} \]
		\item Si $b = (0:1)$, definimos $φ$ por la multiplicación de la matriz:
		\[ A = \begin{pmatrix}1 & -1/a_1\\1 & 0\end{pmatrix} \]
	\end{itemize}
	\item Si $a=(0:1)$. Entonces:
	\begin{itemize}
		\item Si $b = (1:b_1)$, definimos $φ$ por la multiplicación de la matriz:
		\[ A = \begin{pmatrix}0 & 1\\1 & b_1\end{pmatrix} \]
		\item Si $b = (0:1)$, definimos $φ$ por la multiplicación de la matriz:
		\[ A = \begin{pmatrix}1 & 0\\0 & 1\end{pmatrix} \]
	\end{itemize}
\end{itemize}
En cada caso tenemos que $φ(a) = b$ y $φ$ está definida por una multiplicación por una matriz invertible, luego $φ$ es un automorfismo.

\item Supongamos primero que tenemos un automorfismo $φ : \PP_k^1 \to \PP_k^1$ que fija el punto del infinito, es decir, $φ(0:1)=(0:1)$. 
Por biyectividad, $φ$ induce un automorfismo de $\mathbb{A}_k^1$ dado por $\tilde{φ}(x)=φ(1:x)$ visto como punto de $\mathbb{A}_k^1$. Por el ejercicio 5 de la relación 6, podemos escribir $\tilde{φ}(x)=cx+d$ para ciertos $c,d\in k$ con $c \neq 0$. Por lo tanto, podemos escribir $φ$ como la multiplicación por la matriz invertible:
\[ B = \begin{pmatrix}1 & 0\\d & c\end{pmatrix} \]

En el caso general, para un automorfismo $φ$ cualquiera, sea $φ(0:1)=p$. Sabemos, por el apartado anterior, que existe un automorfismo $ψ$ que envía $p$ a $(0:1)$ y que está dado por la multiplicación una matriz $A$. Por otro lado, $ψ \circ φ = χ$ es un automorfismo que fija el punto del infinito, luego lo podemos escribir como la matriz $B$ dada anteriormente. Entonces, componiendo  tenemos que $φ = ψ^{-1} \circ χ$. Ya que $ψ^{-1}$ y $χ$ están dadas por multiplicación de matrices invertibles, tenemos que $φ$ está definida por la multiplicación de la matriz invertible $A^{-1}B$.
\end{itemize}
\end{solucion}

\newpage

\begin{ejercicio}{2} Sea $X \subset \mathbb{P}^n_k$ una variedad proyectiva, $U\subset \mathbb{P}^1_k$ un abierto no vacío y $\phi\colon U\rightarrow X$ un morfismo. Probar que $\phi$ se extiende de manera única a un morfismo $\overline{\phi}\colon \mathbb{P}^1_k\rightarrow X$. Probar que no es cierto el caso en el que $X$ es afín.
\end{ejercicio}
\begin{solucion} 
Sean $x,y\in U$, con $x$ fijo e $y$ arbitrario, entonces $\exists U_x,U_y \subset U$ abiertos tales que $x\in U_x$ e $y\in U_y$, $\exists f_0,f_1$ y $g_0,g_1$ polinomios homogéneos con $\deg(f_i)=n$ $\forall i$ y $\deg(g_i)=m$ $\forall i$, tales que tanto los $f_i$ y los $g_i$ no se anulan simultáneamente en sus respectivos abiertos, teniéndose finalmente que 
$$\phi =(f_0(x_0:x_1):f_1(x_0:x_1)) \quad \forall (x_0:x_1) \in U_x$$ 
$$\phi = (g_0(x_0:x_1):g_1(x_0:x_1)) \quad \forall (x_0:x_1) \in U_y$$
Podemos definir los morfismos $\phi_x$ y $\phi_y$ en $U_x$ y $U_y$ tomando  los propios dominios como abiertos de definición en cada punto y como polinomios los propios $f_i$ y $g_i$ correspondientemente. 

Podemos suponer que los $f_0,f_1$ y los $g_0,g_1$ no se anulan simultáneamente en ningún punto de $\A^2_k$. Veamos el caso de las $f_i$, el otro es análogo. Sabemos que se puede anular simultáneamente en, a lo sumo, una cantidad finita $s_1,\dotsc,s_k$ de puntos proyectivos. Supongamos que $s_i = (a_i:b_i)$. En tal caso, podemos escribir $f_i = \left(\prod_{i=1}^k (b_ix_0-a_ix_1)\right)\cdot f_i'$ para cualesquiera representantes de $s_i$ fijos, y con $f_i'$ polinomios homogéneos del mismo grado. En tal caso podemos tomar los polinomios $(f_0':f_1')$, pues no se anulan simultáneamente en $\PP^1_k$ y coinciden con $(f_0:f_1)$ en $U$. 

Por lo anterior, podemos extender las definiciones de $\phi_x$ y $\phi_y$ a todo $\PP^1_k$. Vamos a ver que de hecho son el mismo morfismo. Como $\phi_x$ coincide con $\phi_y$ en $U_x\cap U_y$, en este dominio $h(z)=f_0g_1(z)$ y $p(z)=f_1g_0(z)$ coinciden. En particular, lo hacen en un número inifnito de puntos afines $(1:a)$, luego $p^{dh}$ y $h^{dh}$ -polinomios en una varaible- coinciden en un número infinito de puntos, lo que prueba que son iguales en todo $\A^1_k$. Por esto mismo, $p \equiv x_0^qh$, pero como $\deg{p}=\deg{h} =n\cdot m$ deducimos que $q=0$, luego coinciden en todo $\A_k^2$. Como $f_1g_0(x_0:x_1)=f_0g_1(x_0:x_1)$ para todo $(x_0:x_1)\in \PP^1_k$ y $\phi_x,\phi_y$ están bien definidas en $\PP^1_k$ en cada punto $\exists i_0,j_0$ tales que $f_{i_0}$ y $g_{j_0}$ no se anulan, entonces $g_j = \frac{g_{j_0}}{f_{i_0}}f_j$. Tomando $j=j_0$, tenemos que $\lambda:=g_{i_0}/f_{j_0}$, tenemos que $g_j = \lambda f_j$ $\forall j$, luego $\phi_x = \phi_y$.
En particular $\phi_x(y)=\phi_y(y)=\phi(y)$. Como $y$ era arbitrario, deducimos que $\phi_x(z)=\phi(z)$ $\forall z \in U$. Como los morfismos proyectivos son continuos, entonces para todo $A$, $\phi(\overline{A})\subset \overline{\phi(A)}$. Aplicándolo a $\phi_x$ con $A=U$, tenemos que
$$
\phi_x(\PP^1_k)=\phi_x(\overline{U}) \subset \overline{\phi_x({U})} \subset \overline{X} =X
$$
Es decir, hemos encontrado un morfismo $\phi_x$ de $\PP^1_k$ en $X$ que coincide con $\phi$ en $U$. Vamos a ver que es único. Supongamos que existe otro morfismo $\psi$ que extiende a $\phi$. En tal caso $\phi_x$ coincide con $\psi$ en todo $U$. Tenemos que ver que coinciden en $\PP^1_k\setminus U$ (una cantidad finita de puntos). Sea $z \in \PP^1_k\setminus U$, entonces $\exists U_z \subset \PP^1_k$ tales que $z\in U_z$ y existen $g_0,g_1$ homogéneos del mismo grado que no se anulan simultáneamente en $\PP^1_k$ tales que
$$
\psi(x_0:x_1) = (g_0(x_0:x_1):g_1(x_0:x_1)) \quad \forall (x_0:x_1)\in U_z
$$
De manera análoga a la prueba de que $\phi_x$ y $\phi_y$ coinciden en $\PP^1_k$, podemos ver que $\phi$  y $\phi_x$ coinciden en $U_z$, (viendo que si $f_0g_1 = f_1g_0$ coinciden en $U\cap U_z$ entonces coinciden en $\PP^1_k$, en particular en $U_z$ donde $\phi = (g_0:g_1)$), lo que prueba que son iguales en todo $\PP^1_k$ (pues $z$ era arbitrario en el complementario de $U$). Tomando $\phi_x = \overline{\phi}$ damos por concluido esta parte del problema.

Resta ver solo que el resultado no es cierto si $X$ es afín. Sea $U = \PP^1_k\setminus\{(0:1)\}$ y $X=\A^1_k$ y el morfismo
$ \phi(x_0:x_1) = \dfrac{x_1}{x_0}$. Por definición de morfismo de una variedad cuasiproyectiva en $\A_k^1$, si se pudiera extender $\phi$, entonces habría de existir una función regular $f\in \mathcal{O}(\PP^1_k)$ tal que $f(x_0:x_1) = \dfrac{x_1}{x_0}$ y además $f$ tendría que estar definida en $(0:1)$. Por el Ejercicio 1.4.9. las funciones regulares de $\PP^1_k$ son precisamente las constantes $A/B$ con $A,B\in k$, luego $f$ no puede coincidir con $\dfrac{x_1}{x_0}$, pues no es constante en $U$ (por ejemplo en $\phi(1:1)=1$ y $\phi(1:0)=0$).


\end{solucion}
\newpage 
\begin{ejercicio}{5}\
\emph{La curva normal racional}. La imagen $X_d$ de la inmersión de
Veronese $\PP^1_k \to \PP^d_k$ para $n = 1$ se llama la \textbf{curva normal racional} en $\PP^d_k$.
\begin{enumerate}
\item Probar que $X_d$ es el conjunto de puntos $(x_0 : x_1 : \dots : x_d) \in \PP^d_k$ tales que
la matriz 
\[
\begin{pmatrix}
x_0 & x_1 &\cdots& x_{d-1}\\
x_1 & x_2 &\cdots& x_d
\end{pmatrix}
\]
tiene rango 1.
\item Probar que $X_d$ es la clausura proyectiva de la imagen del morfismo $\phi :\A^1_k \to \A^d_k$ dado por $\phi(t) = (t, t^2, \dots , t^d)$.
\item Probar que tres puntos distintos de $X_d$ nunca están en la misma recta.
\end{enumerate}
\end{ejercicio}
\begin{solucion}\
Recordemos que la inmersión de Veronese para $n=1$ es $\phi_{1,d}[x_0:x_1]=[x^{\alpha^0}:\dots:x^{\alpha^d}]$, donde $x^{\alpha^i}=x_0^{\alpha^i_0}x_1^{\alpha^i_1}$ y $\alpha^i$ es la $i$-ésima tupla de $S(1,d)$, es decir, la $i$-ésima tupla de enteros negativos que cumple $\alpha_0+\alpha_1=d$. El orden que vamos a establecer va a ser el siguiente: $$(d,0),(d-1,1),\dots, (0,d).$$
Por tanto $x^{\alpha^i}=x_0^{d-i}x_1^i$. Así, $\phi_{1,d}[x_0:x_1]=[x_0^d:x_0^{d-1}x_1:\dots:x_1^d]$
\begin{enumerate}
\item A partir de las aclaraciones anteriores, observamos que $$X_d\subseteq\V(x_0x_2-x_1^2,x_0x_3-x_1x_2,\dots, x_0x_d-x_1x_{d-1},x_1x_3-x_2^2,x_1x_4-x_2x_3,\dots, x_{d-2}x_d-x_{d-1}^2)=X$$
Es decir, verifican las ecuaciones
\begin{align*}
x_ix_{i+2}=x_{i+1}^2\ &\forall i=0,\dots,d-2\\
x_ix_j=x_{i+1}x_{j-1}\ &\forall 0\leq i< j\leq d, j-i>2
\end{align*}
En efecto, $\forall i=0,\dots,d$, $x_i=y_0^{d-i}y_1^i$, luego $$x_ix_{i+2}=y_0^{d-i}y_1^iy_0^{d-i-2}y_1^{i+2}=y_0^{2(d-i-1)}y_1^{2(i+1)}=x_{i+1}^2.$$ Si tomamos $0\leq i<j\leq d$ tales que $j-i>2$, entonces, $$x_ix_{j}=y_0^{d-i}y_1^iy_0^{d-j}y_1^{j}=y_0^{d-i-1}y_1^{i+1}y_0^{d-j+1}y_1^{j-1}=x_{i+1}x_{j-1}.$$
Esos polinomios son precisamente los menores de la matriz del enunciado, que al ser nulos significa que la matriz tiene rango 1.

Recíprocamente, tomemos un punto que haga que la matriz tenga rango 1. Entonces pueden darse dos casos, o que las dos filas de la matriz sean proporcionales siendo todos sus elementos no nulos, o que solo haya un elemento distinto de 0, que debe ser o bien $x_0$ o bien $x_d$ por cómo está construida la matriz. En el primer casos tendríamos $x_1=kx_0, x_2=kx_1=k^2x_0$, etc. para $k\neq 0$. En definitiva, el punto sería de la forma $[x_0:kx_0:k^2x_0:\dots:k^dx_0]$. Como en este caso estamos suponiendo que $x_0\neq 0$, este punto es el mismo que $[1:k:\dots:k^d]=\phi_{1,d}[1:k]$. En el segundo caso, obtendríamos o bien el punto $[1:0:\dots:0]=\phi_{1,d}[1:0]$ o bien $[0:\dots:0:1]=\phi_{1,d}[0:1].$

Esto de hecho prueba la otra inclusión, pues tener rango 1 es equivalente a que sus menores sean 0, por lo que $X_d=X$.

\item En primer lugar observemos que $\Ima{\phi_{1,d}}\cap\A^1=\phi_{1,d}(1:x_1)=[1:x_1:x_1^2:\dots:x_1^d]=\Ima{\varphi}$ vista en como subconjunto proyectivo, por lo que $\overline{\Ima{\varphi}}\subseteq \Ima{\phi_{1,d}}=X_d$. 

Por otra parte, podemos ver que $\Ima\varphi=\V(y_2-y_1^2, y_3-y_1^3,\dots, y_d-y_1^d)=\V(f_1,\dots,f_d)$. La inclusión $\Ima\varphi\subseteq \V(f_1,\dots,f_d)$ es trivial. Sea entonces $\beta=(\beta_1,\dots,\beta_d)\in\V(f_1,\dots,f_d)$. De aquí obtenemos $\beta_i=\beta_1^i\ \forall i=1,\dots, d$, luego $\beta\in\Ima\varphi$.
Además, $\I(\V(f_1,\dots,f_d))=\langle f_1,\dots,f_d\rangle=I$, pues $k[y_1,\dots,y_d]/I\cong k[y_1]$, lo cual implica que $I$ es primo, por lo que $I=\sqrt{I}$. 

Llamemos $$J=\langle x_ix_{i+2}=x_{i+1}^2,\ x_kx_j=x_{k+1}x_{j-1}\ \forall i=0,\dots,d-2\ \ \forall 0\leq k< j\leq d, j-i>2 \rangle.$$ %Entonces $J=\sqrt{J}$ pues $k[x_0,x_1,\dots,x_d]/J\cong k[x_0,x_1]$, que es dominio de integridad. 
Recordemos que $\V(J)=\{[1:t:\dots:t^d], t\in k\}\cup\{[0:\dots:0:1]\}=X_d$ por ser los puntos de la imagen de $\phi_{1,d}$. Vamos a probar que $\V(J)=\overline{\Ima\varphi}$. Supongamos que $f\in k[x_0,x_1,\dots, x_n]$ es homogéneo de grado $n$ que se anula en $\Ima\varphi$. Entonces el polinomio $g(t)=f(1,t,\dots,t^d)$ debe ser el polinomio 0. Si $f(0,\dots,0,1)\neq 0$, entonces $f$ debe contener un término de la forma $ax_d^n$, con $a\neq 0$. Pero entonces $g(t)=at^{dn}+h(t)$ con $\deg h<dn$, lo cual quiere decir que $g$ no puede ser el polinomio 0, con lo que tenemos una contradicción. 

Como $\overline{\Ima\varphi}=\V^{pr}(\{f^h\mid f\in I\})\supseteq\V(J)=X_d$, hemos probado el resultado.

\item Se va a probar un resultado más general, y es que $d+1$ puntos distintos de $X_d$ son siempre linealmente independientes, en particular, 3 de ellos no pueden estar en la misma recta. Sea $\{[a_{i,0}^d:a_{i,0}^{d-1}a_{i,1}:\dots:a_{i,1}^d]\mid i=0,\dots, d\}\subset X_d$ un conjunto de $d+1$ puntos distintos de $X_d$. Que sean linealmente independientes es equivalente a que el determinante
\[
\begin{vmatrix}
a_{0,0}^d&a_{0,0}^{d-1}a_{0,1}&\dots&a_{0,1}^d\\
a_{1,0}^d&a_{1,0}^{d-1}a_{1,1}&\dots&a_{1,1}^d\\
\vdots   &\vdots               &\ddots& \vdots\\
a_{d,0}^d&a_{d,0}^{d-1}a_{d,1}&\dots&a_{d,1}^d        
\end{vmatrix}\neq 0
\]
Puede pasar que alguno de los $a_{i,0}=0$ o que ninguno lo sea. En caso de que ninguno lo sea, como no nos importa el valor numérico del determinante sino simplemente si es cero o no, podemos pasar a considerar el determinante
\[
\begin{vmatrix}
1&\frac{a_{0,1}}{a_{0,0}}&\dots& \left(\frac{a_{0,1}}{a_{0,0}}\right)^d\\
1&\frac{a_{1,1}}{a_{1,0}}&\dots& \left(\frac{a_{1,1}}{a_{1,0}}\right)^d\\
\vdots   &\vdots               &\ddots& \vdots\\
1&\frac{a_{d,1}}{a_{d,0}}&\dots& \left(\frac{a_{d,1}}{a_{d,0}}\right)^d       
\end{vmatrix}
\]
En el caso de que alguno de los $a_{i,0}=0$, su fila será proporcional a $(0,0,\dots,0,1)$, por lo que podemos desarrollar por esa fila y obtener un determinante análogo al anterior de orden uno menos. Es decir, en cualquier caso obtenemos un determinante de la forma
\[
|V|=\begin{vmatrix}
1&\alpha_1&\dots& \alpha_1^{n-1}\\
1&\alpha_2&\dots& \alpha_2^{n-1}\\
\vdots   &\vdots               &\ddots& \vdots\\
1&\alpha_n&\dots& \alpha_n^{n-1}       
\end{vmatrix}
\] 
con $n=d$ o $n=d+1$ según el caso. Sea como fuere, vamos a probar que  $$|V|=\prod_{1\leq i<j\leq n}(\alpha_j-\alpha_i).$$ En particular, cuando los $\alpha_i$ son distintos entre sí, $|V|\neq 0$. Para el caso $n=2$ es fácil ver que $|V|=\alpha_2-\alpha_1=\prod_{1\le i<j\le 2} (\alpha_j-\alpha_i)$. Vamos al caso general $n\times n$. En este caso hacemos la operación elemental $C_j\rightarrow C_j-(\alpha_1 C_{j-1})\ \forall j=2,\dots, n$, cuyo resultado es
\[
|V|=\begin{vmatrix}
1 & 0 & 0 & \dots & 0\\
1 & \alpha_2-\alpha_1 & \alpha_2(\alpha_2-\alpha_1) & \dots & \alpha_2^{n-2}(\alpha_2-\alpha_1)\\
1 & \alpha_3-\alpha_1 & \alpha_3(\alpha_3-\alpha_1) & \dots & \alpha_3^{n-2}(\alpha_3-\alpha_1)\\
\vdots & \vdots & \vdots & \ddots &\vdots \\
1 & \alpha_n-\alpha_1 & \alpha_n(\alpha_n-\alpha_1) & \dots & \alpha_n^{n-2}(\alpha_n-\alpha_1)\\
\end{vmatrix}
\]
Desarrollando por la primera fila obtenemos el determinante $(n-1)\times(n-1)$
\[
\begin{vmatrix} V \end{vmatrix}=
(\alpha_2-\alpha_1)(\alpha_3-\alpha_1)\cdots(\alpha_n-\alpha_1)
\begin{vmatrix}
1 & \alpha_2 & \alpha_2^2 & \dots & \alpha_2^{n-2}\\
1 & \alpha_3 & \alpha_3^2 & \dots & \alpha_3^{n-2}\\
\vdots & \vdots & \vdots &\ddots &\vdots \\
1 & \alpha_n & \alpha_n^2 & \dots & \alpha_n^{n-2}\\
\end{vmatrix}
\]
Volvemos a hacer el mismo proceso en ese determinante, esta vez $C_j\rightarrow C_j-(\alpha_2 C_{j-1})\ \forall j=2,\dots, n-1$, de donde obtendríamos
\[
|V|=(\alpha_2-\alpha_1)\cdots(\alpha_n-\alpha_1)\begin{vmatrix}
1 & 0 & 0 & \dots & 0\\
1 & \alpha_3-\alpha_2 & \alpha_3(\alpha_3-\alpha_2)& \dots & \alpha_3^{n-3}(\alpha_3-\alpha_2)\\
\vdots & \vdots & \vdots & \ddots&\vdots \\
1 & \alpha_n-\alpha_2 & \alpha_n(\alpha_n-\alpha_2) & \dots & \alpha_n^{n-3}(\alpha_n-\alpha_2)\\
\end{vmatrix}
\]
Continuando este proceso hasta llegar al caso $2\times 2$ se prueba el resultado.
\end{enumerate}
\end{solucion}
\newpage 

\end{document}