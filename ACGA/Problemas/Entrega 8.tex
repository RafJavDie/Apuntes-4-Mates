\documentclass[twoside]{article}
\usepackage{../../estilo-ejercicios}
\DeclareMathOperator{\Ima}{Im}

%--------------------------------------------------------
\begin{document}

\title{Algebra Conmutativa y Geometría Aplicada}
\author{Javier Aguilar Martín, Rafael González López}
\maketitle
\begin{ejercicio}{2} Sea $X \subset \mathbb{P}^n_k$ una variedad proyectiva, $U\subset \mathbb{P}^1_k$ un abierto no vacío y $\phi\colon U\rightarrow X$ un morfismo. Probar que $\phi$ se extiende de manera única a un morfismo $\overline{\phi}\colon \mathbb{P}^1_k\rightarrow X$. Probar que no es cierto el caso en el que $X$ es afín.
\end{ejercicio}
\begin{solucion} 
Sean $x,y\in U$, con $x$ fijo e $y$ arbitrario, entonces $\exists U_x,U_y \subset U$ abiertos tales que $x\in U_x$ e $y\in U_y$, $\exists f_0,f_1$ y $g_0,g_1$ polinomios homogéneos con $\deg(f_i)=n$ $\forall i$ y $\deg(g_i)=m$ $\forall i$, tales que tanto los $f_i$ y los $g_i$ no se anulan simultáneamente en sus respectivos abiertos, teniéndose finalmente que 
$$\phi =(f_0(x_0:x_1):f_1(x_0:x_1)) \quad \forall (x_0:x_1) \in U_x$$ 
$$\phi = (g_0(x_0:x_1):g_1(x_0:x_1)) \quad \forall (x_0:x_1) \in U_y$$
Podemos definir los morfismos $\phi_x$ y $\phi_y$ en $U_x$ y $U_y$ tomando  los propios dominios como abiertos de definición en cada punto y como polinomios los propios $f_i$ y $g_i$ correspondientemente. 

Podemos suponer que los $f_0,f_1$ y los $g_0,g_1$ no se anulan simultáneamente en ningún punto de $\A^2_k$. Veamos el caso de las $f_i$, el otro es análogo. Sabemos que se puede anular simultáneamente en, a lo sumo, una cantidad finita $s_1,\dotsc,s_k$ de puntos proyectivos. Supongamos que $s_i = (a_i:b_i)$. En tal caso, podemos escribir $f_i = \left(\prod_{i=1}^k (b_ix_0-a_ix_1)\right)\cdot f_i'$ para cualesquiera representantes de $s_i$ fijos, y con $f_i'$ polinomios homogéneos del mismo grado. En tal caso podemos tomar los polinomios $(f_0':f_1')$, pues no se anulan simultáneamente en $\PP^1_k$ y coinciden con $(f_0:f_1)$ en $U$. 

Por lo anterior, podemos extender las definiciones de $\phi_x$ y $\phi_y$ a todo $\PP^1_k$. Vamos a ver que de hecho son el mismo morfismo. Como $\phi_x$ coincide con $\phi_y$ en $U_x\cap U_y$, en este dominio $h(z)=f_0g_1(z)$ y $p(z)=f_1g_0(z)$ coinciden. En particular, lo hacen en un número inifnito de puntos afines $(1:a)$, luego $p^{dh}$ y $h^{dh}$ -polinomios en una varaible- coinciden en un número infinito de puntos, lo que prueba que son iguales en todo $\A^1_k$. Por esto mismo, $p \equiv x_0^qh$, pero como $\deg{p}=\deg{h} =n\cdot m$ deducimos que $q=0$, luego coinciden en todo $\A_k^2$. Utilizando un procedimiento análogo al de la demostración de la Proposición 1.7.2, podemos deducir que $\phi_x$ y $\phi_y$ también coinciden en $\PP^1_k$. 

En particular $\phi_x(y)=\phi_y(y)=\phi(y)$. Como $y$ era arbitrario, deducimos que $\phi_x(z)=\phi(z)$ $\forall z \in U$. Como los morfismos proyectivos son continuos, entonces para todo $A$, $\phi(\overline{A})\subset \overline{\phi(A)}$. Aplicándolo a $\phi_x$ con $A=U$, tenemos que
$$
\phi_x(\PP^1_k)=\phi_x(\overline{U}) \subset \overline{\phi_x({U})} \subset \overline{X} =X
$$
Es decir, hemos encontrado un morfismo $\phi_x$ de $\PP^1_k$ en $X$ que coincide con $\phi$ en $U$. Vamos a ver que es único. Supongamos que existe otro morfismo $\psi$ que extiende a $\phi$. En tal caso $\phi_x$ coincide con $\psi$ en todo $U$. Tenemos que ver que coinciden en $\PP^1_k\setminus U$ (una cantidad finita de puntos). Sea $z \in \PP^1_k\setminus U$, entonces $\exists U_z \subset \PP^1_k$ tales que $z\in U_z$ y existen $g_0,g_1$ homogéneos del mismo grado que no se anulan simultáneamente en $\PP^1_k$ tales que
$$
\psi(x_0:x_1) = (g_0(x_0:x_1):g_1(x_0:x_1)) \quad \forall (x_0:x_1)\in U_z
$$
De manera análoga a la prueba de que $\phi_x$ y $\phi_y$ coinciden en $\PP^1_k$, podemos ver que $\phi$  y $\phi_x$ coinciden en $U_z$, (viendo que si $f_0g_1 = f_1g_0$ coinciden en $U\cap U_z$ entonces coinciden en $\PP^1_k$, en particular en $U_z$ donde $\phi = (g_0:g_1)$), lo que prueba que son iguales en todo $\PP^1_k$ (pues $z$ era arbitrario en el complementario de $U$). Tomando $\phi_x = \overline{\phi}$ damos por concluido esta parte del problema.

Resta ver solo que el resultado no es cierto si $X$ es afín. Sea $U = \PP^1_k\setminus\{(0:1)\}$ y $X=\A^1_k$ y el morfismo
$ \phi(x_0:x_1) = \dfrac{x_1}{x_0}$. Por definición de morfismo de una variedad cuasiproyectiva en $\A_k^1$, habría de existir una función regular $f\in \mathcal{O}(\PP^1_k)$ tal que $f(x_0:x_1) = \dfrac{x_1}{x_0}$ y además $f$ está definida en $(0:1)$. Por el Ejercicio 1.4.9. las funciones regulares de $\PP^1_k$ son precisamente las constantes $A/B$ con $A,B\in k$, luego $f$ no puede coincidir con $\dfrac{x_1}{x_0}$, pues no es constante en $U$ (por ejemplo en $(1:1)$ y $(1:2)$).


\end{solucion}
\newpage 
\begin{ejercicio}{5}\
\emph{La curva normal racional}. La imagen $X_d$ de la inmersión de
Veronese $\PP^1_k \to \PP^d_k$ para $n = 1$ se llama la \textbf{curva normal racional} en $\PP^d_k$.
\begin{enumerate}
\item Probar que $X_d$ es el conjunto de puntos $(x_0 : x_1 : \dots : x_d) \in \PP^d_k$ tales que
la matriz 
\[
\begin{pmatrix}
x_0 & x_1 &\cdots& x_{d-1}\\
x_1 & x_2 &\cdots& x_d
\end{pmatrix}
\]
tiene rango 1.
\item Probar que $X_d$ es la clausura proyectiva de la imagen del morfismo $\phi :\A^1_k \to \A^d_k$ dado por $\phi(t) = (t, t^2, \dots , t^d)$.
\item Probar que tres puntos distintos de $X_d$ nunca están en la misma recta.
\end{enumerate}
\end{ejercicio}
\begin{solucion}\
Recordemos que la inmersión de Veronese para $n=1$ es $\phi_{1,d}[x_0:x_1]=[x^{\alpha^0}:\dots:x^{\alpha^d}]$, donde $x^{\alpha^i}=x_0^{\alpha^i_0}x_1^{\alpha^i_1}$ y $\alpha^i$ es la $i$-ésima tupla de $S(1,d)$, es decir, la $i$-ésima tupla de enteros negativos que cumple $\alpha_0+\alpha_1=d$. El orden que vamos a establecer va a ser el siguiente: $$(d,0),(d-1,1),\dots, (0,d).$$
Por tanto $x^{\alpha^i}=x_0^{d-i}x_1^i$. Así, $\phi_{1,d}[x_0:x_1]=[x_0^d:x_0^{d-1}x_1:\dots:x_1^d]$
\begin{enumerate}
\item A partir de las aclaraciones anteriores, observamos que $$X_d\subseteq\V(x_0x_2-x_1^2,x_0x_3-x_1x_2,\dots, x_0x_d-x_1x_{d-1},x_1x_3-x_2^2,x_1x_4-x_2x_3,\dots, x_{d-2}x_d-x_{d-1}^2)=X$$
Es decir, verifican las ecuaciones
\begin{align*}
x_ix_{i+2}=x_{i+1}^2\ &\forall i=0,\dots,d-1\\
x_ix_j=x_{i+1}x_{j-1}\ &\forall 0\leq i< j\leq d, j-i>2
\end{align*}
En efecto, $\forall i=0,\dots,d$, $x_i=y_0^{d-i}y_1^i$, luego $$x_ix_{i+2}=y_0^{d-i}y_1^iy_0^{d-i-2}y_1^{i+2}=y_0^{2(d-i-1)}y_1^{2(i+1)}=x_{i+1}^2.$$ Si tomamos $0\leq i<j\leq d$ tales que $j-i>2$, entonces, $$x_ix_{j}=y_0^{d-i}y_1^iy_0^{d-j}y_1^{j}=y_0^{d-i-1}y_1^{i+1}y_0^{d-j+1}y_1^{j-1}=x_{i+1}x_{j-1}.$$
Esos polinomios son precisamente los menores de la matriz del enunciado, que al ser nulos significa que la matriz tiene rango 1.

Recíprocamente, tomemos un punto que haga que la matriz tenga rango 1. Entonces pueden darse dos casos, o que las dos filas de la matriz sean proporcionales siendo todos sus elementos no nulos, o que solo haya un elemento distinto de 0, que debe ser o bien $x_0$ o bien $x_d$ por cómo está construida la matriz. En el primer casos tendríamos $x_1=kx_0, x_2=kx_1=k^2x_0$, etc. para $k\neq 0$. En definitiva, el punto sería de la forma $[x_0:kx_0:k^2x_0:\dots:k^dx_0]$. Como en este caso estamos suponiendo que $x_0\neq 0$, este punto es el mismo que $[1:k:\dots:k^d]=\phi_{1,d}[1:k]$. En el segundo caso, obtendríamos o bien el punto $[1:0:\dots:0]=\phi_{1,d}[1:0]$ o bien $[0:\dots:0:1]=\phi_{1,d}[0:1].$

Esto de hecho prueba la otra inclusión, pues tener rango 1 es equivalente a que sus menores sean 0, por lo que $X_d=X$.

\item En primer lugar observemos que $\Ima{\phi_{1,d}}\cap\A^1=\phi_{1,d}(1:x_1)=[1:x_1:x_1^2:\dots:x_1^d]=\Ima{\varphi}$ vista en como subconjunto proyectivo, por lo que $\overline{\Ima{\varphi}}\subseteq \Ima{\phi_{1,d}}=X_d$. 

Por otra parte, podemos ver que $\Ima\varphi=\V(y_2-y_1^2, y_3-y_1^3,\dots, y_d-y_1^d)=\V(f_1,\dots,f_d)$. La inclusión $\Ima\varphi\subseteq \V(f_1,\dots,f_d)$ es trivial. Sea entonces $\beta=(\beta_1,\dots,\beta_d)\in\V(f_1,\dots,f_d)$. De aquí obtenemos $\beta_i=\beta_1^i\ \forall i=1,\dots, d$, luego $\beta\in\Ima\varphi$.
Además, $\I(\V(f_1,\dots,f_d))=\langle f_1,\dots,f_d\rangle=I$, pues $k[y_1,\dots,y_d]/I\cong k[y_1]$, lo cual implica que $I$ es primo, por lo que $I=\sqrt{I}$. 

Llamemos $$J=\langle x_ix_{i+2}=x_{i+1}^2,\ x_kx_j=x_{k+1}x_{j-1}\ \forall i=0,\dots,d-1\ \ \forall 0\leq k< j\leq d, j-i>2 \rangle.$$ %Entonces $J=\sqrt{J}$ pues $k[x_0,x_1,\dots,x_d]/J\cong k[x_0,x_1]$, que es dominio de integridad. 
Recordemos que $\V(J)=\{[1:t:\dots:t^d], t\in k\}\cup\{[0:\dots:0:1]\}=X_d$ por ser los puntos de la imagen de $\phi_{1,d}$. Vamos a probar que $\V(J)=\overline{\Ima\varphi}$. Supongamos que $f\in k[x_0,x_1,\dots, x_n]$ es homogéneo de grado $n$ que se anula en $\Ima\varphi$. Entonces el polinomio $g(t)=f(1,t,\dots,t^d)$ debe ser el polinomio 0. Si $f(0,\dots,0,1)\neq 0$, entonces $f$ debe contener un término de la forma $ax_d^n$, con $a\neq 0$. Pero entonces $g(t)=at^{dn}+h(t)$ con $\deg h<dn$, lo cual quiere decir que $g$ no puede ser el polinomio 0, con lo que tenemos una contradicción. 

Como $\overline{\Ima\varphi}=\V^{pr}(\{f^h\mid f\in I\})\supseteq\V(J)=X_d$, hemos probado el resultado.

\item Se va a probar un resultado más general, y es que $d+1$ puntos distintos de $X_d$ son siempre linealmente independientes, en particular, 3 de ellos no pueden estar en la misma recta. Sea $\{[a_{i,0}^d:a_{i,0}^{d-1}a_{i,1}:\dots:a_{i,1}^d]\mid i=0,\dots, d\}\subset X_d$ un conjunto de $d+1$ puntos distintos de $X_d$. Que sean linealmente independientes es equivalente a que el determinante
\[
\begin{vmatrix}
a_{0,0}^d&a_{0,0}^{d-1}a_{0,1}&\dots&a_{0,1}^d\\
a_{1,0}^d&a_{1,0}^{d-1}a_{1,1}&\dots&a_{1,1}^d\\
\vdots   &\vdots               &\ddots& \vdots\\
a_{d,0}^d&a_{d,0}^{d-1}a_{d,1}&\dots&a_{d,1}^d        
\end{vmatrix}\neq 0
\]
Puede pasar que alguno de los $a_{i,0}=0$ o que ninguno lo sea. En caso de que ninguno lo sea, como no nos importa el valor numérico del determinante sino simplemente si es cero o no, podemos pasar a considerar el determinante
\[
\begin{vmatrix}
1&\frac{a_{0,1}}{a_{0,0}}&\dots& \left(\frac{a_{0,1}}{a_{0,0}}\right)^d\\
1&\frac{a_{1,1}}{a_{1,0}}&\dots& \left(\frac{a_{1,1}}{a_{1,0}}\right)^d\\
\vdots   &\vdots               &\ddots& \vdots\\
1&\frac{a_{d,1}}{a_{d,0}}&\dots& \left(\frac{a_{d,1}}{a_{d,0}}\right)^d       
\end{vmatrix}
\]
En el caso de que alguno de los $a_{i,0}=0$, su fila será proporcional a $(0,0,\dots,0,1)$, por lo que podemos desarrollar por esa fila y obtener un determinante análogo al anterior de orden uno menos. Es decir, en cualquier caso obtenemos un determinante de la forma
\[
|V|=\begin{vmatrix}
1&\alpha_1&\dots& \alpha_1^{n-1}\\
1&\alpha_2&\dots& \alpha_1^{n-1}\\
\vdots   &\vdots               &\ddots& \vdots\\
1&\alpha_n&\dots& \alpha_n^{n-1}       
\end{vmatrix}
\] 
con $n=d$ o $n=d+1$ según el caso. Sea como fuere, vamos a probar que  $$|V|=\prod_{1\leq i<j\leq n}(\alpha_j-\alpha_i).$$ En particular, cuando los $\alpha_i$ son distintos entre sí, $|V|\neq 0$. Para el caso $n=2$ es fácil ver que $|V|=\alpha_2-\alpha_1=\prod_{1\le i<j\le 2} (\alpha_j-\alpha_i)$. Vamos al caso general $n\times n$. En este caso hacemos la operación elemental $C_j\rightarrow C_j-(\alpha_1 C_{j-1})\ \forall j=2,\dots, n$, cuyo resultado es
\[
|V|=\begin{vmatrix}
1 & 0 & 0 & \dots & 0\\
1 & \alpha_2-\alpha_1 & \alpha_2(\alpha_2-\alpha_1) & \dots & \alpha_2^{n-2}(\alpha_2-\alpha_1)\\
1 & \alpha_3-\alpha_1 & \alpha_3(\alpha_3-\alpha_1) & \dots & \alpha_3^{n-2}(\alpha_3-\alpha_1)\\
\vdots & \vdots & \vdots & \ddots &\vdots \\
1 & \alpha_n-\alpha_1 & \alpha_n(\alpha_n-\alpha_1) & \dots & \alpha_n^{n-2}(\alpha_n-\alpha_1)\\
\end{vmatrix}
\]
Desarrollando por la primera fila obtenemos el determinante $(n-1)\times(n-1)$
\[
\begin{vmatrix} V \end{vmatrix}=
(\alpha_2-\alpha_1)(\alpha_3-\alpha_1)\cdots(\alpha_n-\alpha_1)
\begin{vmatrix}
1 & \alpha_2 & \alpha_2^2 & \dots & \alpha_2^{n-2}\\
1 & \alpha_3 & \alpha_3^2 & \dots & \alpha_3^{n-2}\\
\vdots & \vdots & \vdots &\ddots &\vdots \\
1 & \alpha_n & \alpha_n^2 & \dots & \alpha_n^{n-2}\\
\end{vmatrix}
\]
Volvemos a hacer el mismo proceso en ese determinante, esta vez $C_j\rightarrow C_j-(\alpha_2 C_{j-1})\ \forall j=2,\dots, n-1$, de donde obtendríamos
\[
|V|=(\alpha_2-\alpha_1)\cdots(\alpha_n-\alpha_1)\begin{vmatrix}
1 & 0 & 0 & \dots & 0\\
1 & \alpha_3-\alpha_2 & \alpha_3(\alpha_3-\alpha_2)& \dots & \alpha_3^{n-3}(\alpha_3-\alpha_2)\\
\vdots & \vdots & \vdots & \ddots&\vdots \\
1 & \alpha_n-\alpha_2 & \alpha_n(\alpha_n-\alpha_2) & \dots & \alpha_n^{n-3}(\alpha_n-\alpha_2)\\
\end{vmatrix}
\]
Continuando este proceso hasta llegar al caso $2\times 2$ se prueba el resultado.
\end{enumerate}
\end{solucion}
\newpage 

\end{document}