\documentclass[twoside]{article}
\usepackage{../../estilo-ejercicios}
\newcommand{\media}[1]{{\overline{#1}}}
\newcommand{\muestra}[1]{{\underline{#1}}}
\newcommand{\m}[1]{{\muestra{#1}}}
\newcommand{\mX}{{\muestra{X}}}

%--------------------------------------------------------
\begin{document}

\title{Ejercicios de Análisis de Datos Multivariantes}
\author{Diego Pedraza López, Javier Aguilar Martín, Rafael González López}
\maketitle

\begin{ejercicio}{1.1}
Sea $\mX_1, \mX_2, \dots, \mX_n$ una muestra aleatoria simple de una vector $p$-dimensional $\mX \sim (\m{µ},Σ)$.
Sea $\media{\m{X}} = \frac{1}{n}\sum_{i=1}^n \m{X}_i$ y $S$ la matriz $S = [S_{jk}]$ con
\[ S_{jk} = \sum_{i=1}^n (X_{ij} - \media{X}_j)(X_{ik}-\media{X}_k), \quad j,k=1,\dots,p\]
\begin{enumerate}[(a)]
\item Demuestra que $S = \sum_{i=1}^n (\mX_i-\media{\m{X}})(\mX_i-\media{\m{X}})^t$.
\item Sea $A_{m\times p}$ y sea $\m{Y}_1,\dots,\m{Y}_n$ con $\m{Y}_i = A \m{X}_i$.

Demuestra que $\media{\m{Y}}=A \media{\mX}$ y que $S_Y = A S_X A^t$.
\item Sea $\mathbb{X}$ la matriz de datos. Demuestra que $S = \mathbb{X}^t H \mathbb{X}$, siendo $H$ simétrica e idempotente.
\item Demuestra que $E(\media{\mX}) = \m{μ}$ y $Cov(\media{\mX}) = \frac{1}{n} Σ$.
\item Sea $\hat{Σ} = \frac{1}{n-1}S$, demuestra que $E 	(\hat{Σ}) = Σ$.
\end{enumerate}
\end{ejercicio}
\begin{solucion}
\begin{enumerate}[(a)]
\item[]
\item
\[ \left(\sum_{i=1}^n (\mX_i-\media{\m{X}})(\mX_i-\media{\m{X}})^t\right)_{jk} = \sum_{i=1}^n (\mX_i-\media{\m{X}})_j(\mX_i-\media{\m{X}})_k = S_{jk} \]
\item
\[ \media{\m{Y}} = \frac{1}{n} \sum_{i=1}^n \m{Y}_i = \frac{1}{n} \sum_{i=1}^n A\m{X}_i = A \media{\mX} \]
\[ S_Y = \sum_{i=1}^n (\m{Y}_i-\media{\m{Y}})(\m{Y}_i-\media{\m{Y}})^t = \sum_{i=1}^n (A\mX_i-A\media{\mX})(A\mX_i-A\media{\mX})^t = A S_X A^t\]
\item Recordemos que $H = I_n - \frac{1}{n}E_n^t$, donde $E_n$ es una matriz $n \times n$ de unos. 
Por simetría e idempotencia: $H^t H = H^2 = H$. Entonces:
\[ S = \tilde{\mathbb{X}}^t\tilde{\mathbb{X}} = (H \mathbb{X})^t (H \mathbb{X}) = \mathbb{X}^t H^t H \mathbb{X} = \mathbb{X}^t H \mathbb{X} \]
\item
\[ E(\media{\mX}) = \frac{1}{n}\sum_{i=1}^n E(\mX) = \frac{1}{n} (n \m{µ}) = \m{µ} \]
Como $\mX= \sum_{i=1}^n \muestra{X_i}/n$, y cada muestra es independiente, deducimos 
\begin{gather*}
 Cov(\media{\mX}) = \Sigma_{\media{\mX}}=\sum_{i=1}^n\Sigma_{\muestra{X_i}/n} = \sum_{i=1}^n\frac{1}{n} \Sigma_{\muestra{X_i}} \frac{1}{n} = \frac{1}{n^2}\sum_{i=1}^n \Sigma_{\muestra{X}}= \frac{1}{n}\Sigma
\end{gather*}
\item
\[ E(\hat{Σ})_{jk} = \frac{1}{n-1} E[S_{jk}] = \frac{1}{n-1} \sum_{i=1}^n E\left((X_{ij}-\media{X}_j)(X_{ik}-\media{X}_k)\right)\]
\end{enumerate}

\end{solucion}

\newpage

\begin{ejercicio}{1.2}
Distribución normal $p$-variante, $\mX \sim N_p(\m{μ},Σ)$.
\begin{enumerate}[(a)]
\item Definición.
\item Sea $\m{Y} = Σ^{-1/2}(\mX - \m{μ})$.
\begin{enumerate}[i.]
  \item Determinar la función de densidad de $\m{Y}$.
  \item Demuestra que las componentes del vector aleatorio $\m{Y}$ son independientes e idénticamente distribuidas según una $N(0,1)$.
  \item Demuestra que $\m{Y} \sim N_p(\m{0},I_p)$.
\end{enumerate}
\item Demuestra que $E(\mX) = \m{μ}$ y $cov(\m{X}) = Σ$.
\item La función generatriz de momentos de $\m{X}$ viene dada por
\[ M(\m{t}) = E\left(e^{\m{t}'\mX}\right) = E\left(e^{\sum_{i=1}^p t_i X_i}\right) = \exp\left(\m{t}'\m{μ} + \frac{1}{2}\m{t}'Σ\m{t}\right)\]
Demuestra que
\begin{enumerate}[i.]
\item $B \mX + \m{c} \sim N(B \m{μ} + \m{c}, B Σ B^t)$.
\item Si $\mX \sim N_p(\m{μ}_1,Σ_1)$, $\m{Y} \sim N_p(\m{μ}_2,Σ_2)$ e independientes, entonces $\mX+\m{Y} \sim N_p(\m{μ}_1+\m{μ}_2, Σ_1+Σ_2)$.
\end{enumerate}
\item Sea $\mX_1,\dots,\mX_n$ una muestra aleatoria de una distribución $N_p(\m{μ},Σ)$, demuestra que $\media{\m{X}} \sim N_p(\m{μ}, \frac{1}{n}Σ)$.
\end{enumerate}
\end{ejercicio}
\begin{solucion}
\begin{enumerate}[(a)]
\item[]
\item La distribución normal $p$-variante es la generalización a dimensión $p$ de la distribución normal. Una posible definición es que un vector aleatorio sigue una distribución normal $p$-variante si toda combinación lineal de sus componentes sigue una distribución normal. Como estamos suponiendo que $\Sigma$ es no singular, también es equivalente a que el vector tenga la función de densidad
$$
f(\muestra{x})=\frac{1}{(2\pi)^{p/2}\abs{\Sigma}^{1/2}}\exp\{-1/2(\muestra{x}-\muestra{\mu})'\Sigma^{-1}(\muestra{x}-\muestra{\mu})\}
$$
\item Imagino que al menos para el primer apartado podemos partir de lo que viene en los apuntes
\begin{enumerate}[i.]
\item Sabemos que la densidad de $X$ es
$$
f_X(\muestra{x})=\frac{1}{(2\pi)^{p/2}\abs{\Sigma}^{1/2}}\exp\{-1/2(\muestra{x}-\muestra{\mu})'\Sigma^{-1}(\muestra{x}-\muestra{\mu})\}
$$
Deducimos entonces que
$$
F_Y(\muestra{x})=P[Y\leq \muestra{x}]= P[X\leq \Sigma^{1/2}\muestra x+\mu] = F_X( \Sigma^{1/2}\muestra x+\mu)
$$
Derivando y sustituyendo tenemos
\begin{gather*}
f_Y(x)=f_X(\Sigma^{1/2}\muestra x+\mu)\Sigma^{1/2} = \frac{1}{(2\pi)^{p/2}}\exp\{-1/2x'x\}
\end{gather*}
\item Veamos la primera componente, el resto es análogo
\begin{gather*}
f_{X_i}(x_1)=\int_{-\infty}^\infty \cdots \int_{-\infty}^\infty x_2\cdots x_p f_X(\muestra{x})dx_2\cdots dx_p\\
= \frac{1}{(2\pi)^{p/2}}\int_{-\infty}^\infty \cdots \int_{-\infty}^\infty x_2\cdots x_p\exp\{-1/2(x_1^2+x_2^2+\cdots x_p^2)\}dx_2\cdots dx_p \\
= \frac{\sqrt{2\pi}}{(2\pi)^{p/2}}\int_{-\infty}^\infty \cdots \int_{-\infty}^\infty x_3\cdots x_p\exp\{-1/2(x_1^2+x_3^2\cdots x_p^2)\}dx_3\cdots dx_p \\
= \frac{1}{\sqrt{2\pi}}e^{-x_1^2/2} \sim N(0,1)
\end{gather*}
La independencia se tiene de manera trivial, pues
$$
f_X(\muestra{x})=\prod_{i=1}^p f_{X_i}(x_i)
$$
\item Se tendría por definición.
\end{enumerate}
\item Intuyo que trivialérrimo, aunque si quieres me pongo a integrar.
\item 
\begin{enumerate}[i.]
\item Se deduce inmediatamente a partir de la siguiente cadena de igualdades
\[ E\left(e^{\m{t}'(B\mX+\muestra{c})}\right) = e^{t'\muestra{c}}E\left(e^{t'B\mX}\right) = e^{t'\muestra{c}}M_X(B't) = \exp\left(t'(B\muestra{\mu}+\muestra{c})+\frac{1}{2}t'B\Sigma B't\right)\]
\item Si dos vectores aleatorios son independientes entonces la esperanza del producto es el producto de las esperanzas. Deducimos por tanto que la función generatriz de la suma es el producto de las funciones generatrices, de donde se deduce trivialmente el resultado.
\end{enumerate}
\item Los parámetros de una normal $p$-variante son el vector de medias y la matriz de covarianzas. Sabemos, por el apartado anterior, que 
$$
n\media{\mX}\sim N_p(n\muestra{\mu},n\Sigma)
$$
Es trivial comprobar utilizando también el apartado anterior que para la normal $p$-variante también se verifica que $a N_p(\muestra{\mu},\Sigma)= N_p(a\muestra{\mu},a^2\Sigma)$, de donde se deduce el resultado.
\end{enumerate}
\end{solucion}
\newpage

\begin{ejercicio}{1.3}
Distribución de Wishart, $W \sim W_p(n,Σ)$.
\begin{enumerate}[(a)]
\item Definición.
\item Demuestra que $E(W) = nΣ$.
\item Sean $W_1 \sim W_p(n_1,Σ)$ y $W_2 \sim W_p(n_2,Σ)$, e independientes. Demuestra que $W_1 + W_2 \sim W_p(n_1+n_2,Σ)$.
\item Sea $W \sim W_p(n,Σ)$, y sea $C \in \mathcal{M}_{q\times p}$ de rango $q$. Demuestra que
\[ C W C' \sim W_q(n, CΣC') \]
\end{enumerate}
\end{ejercicio}
\begin{solucion}
\begin{enumerate}[(a)]
\item[]
\item Supongamos que $X$ es una matriz $n\times p$ de manera que las filas de $X$ fomran una muestra aleatoria de una $N_p(0,\Sigma)$. Decimos entonces que $w=X'X$ sigue una distribución de Wishart no singular $p$-dimensional con $n$ grados de libertad.
\item Sabemos que existe $X=[\muestra{X_1},\dotsc,\muestra{X_n}]'$ con $\muestra{X_i}\sim N_p(0,\Sigma)$ independientes entre sí tal que $W=X'X$.
$$
E(W)= E\left(\sum_{i=1}^n \muestra{X_i}\muestra{X_i}'\right) = \sum_{i=1}^n E[\muestra{X_i}\muestra{X_i}'] = \sum_{i=1}^n \Sigma-\mu\mu' \overset{\mu=0}{=} n\Sigma
$$
\item Para $W_i$ tenemos que existen $ X^i=[\muestra{X_1^i},\dotsc,\muestra{X_{n_i}^i}]'$  con $\muestra{X_i}\sim N_p(0,\Sigma)$ independientes entre sí tal que $W_i={X^i}' X^i$. Como la muestras son independientes entre sí, podemos considerar que tenemos una única muestra de tamaño $n_1+n_2$, de donde se deduce trivialmente el resultado.
\item Tenemos que 
$$
CWC' = C\sum_{i=1}^n \muestra{X_i X_i'}C' = \sum_{i=1}^n (C\muestra{X_i}) (C\muestra{X_i})'
$$
Como $CX_i \sim N_p(0,C\Sigma C')$ tenemos el resultado.
\end{enumerate}
\end{solucion}
\newpage

\begin{ejercicio}{1.4}
Distribución $T^2$ de Hotelling, $T^2 \sim T^2_{p,n}$.
\begin{enumerate}[(a)]
\item Definición.
\item Sea $\mX \sim N_p(\m{μ},Σ)$ y $W \sim W_p(n,Σ)$ independientes. Demuestra que:
\[ n(\mX-\m{μ})' W^{-1} (\mX-\m{μ}) \sim T^2_{p,n} \]
\end{enumerate}
\end{ejercicio}
\begin{solucion}
\begin{enumerate}[(a)]
\item[]
\item Sean $\muestra{d}\sim N_p(0,I_p)$ y $W\sim W_p(n,I_p)$ independientes. Se denomina $T^2$ de Hotelling a la distribución del estadístico 
$$
T^2= n\muestra{d}'W^{-1}\muestra{d}
$$
\item Se tiene de manera trivial que
$$
n(\mX-\m{μ})' W^{-1} (\mX-\m{μ}) =n(\Sigma^{-1/2}(\mX-\m{μ}))'(\Sigma^{-1/2} W \Sigma^{-1/2})^{-1}(\Sigma^{-1/2} (\mX-\m{μ}))
$$
Por los ejercicios anteriores sabemos que $(\Sigma^{-1/2}(\mX-\m{μ}))' \sim N(0,I_p)$ y además, usando que $\Sigma^{1/2}$ es simétrica, $\Sigma^{-1/2} W \Sigma^{-1/2} \sim W_p(n,I_p)$.
\end{enumerate}
\end{solucion}
\newpage

\begin{ejercicio}{1.5}
Sea $\mX_1,\dots,\mX_n$ una muestra aleatoria de una distribución $N_p(\m{μ},Σ)$.
\begin{enumerate}[(a)]
\item Sabiendo que $\media{\mX}$ y $S$ son independientes y que $S \sim W_p(n-1,Σ)$, demuestra que
\[ n(n-1)(\media{\mX}-\m{μ})' S^{-1} (\media{\mX}-\m{μ}) \sim T^2_{p,n-1}\]
\item Sea $A$ una matriz $m\times p$ ($m≤p$), de rang $m$ y $\m{d}$ un vector $m$-dimensional.
El estadístico del test de razón de verosimilitudes para el contraste:
\[ H_0 \colon A\m{μ} = \m{d} \quad vs \quad H_1 \colon A\m{μ} \neq \m{d} \]
viene dado por:
\[ T^2 = (n-1)n (A\media{\mX}-\muestra{d})^t (ASA^t)^{-1} (A \media{\mX}-\m{d}) \]
Demuestra que, bajo $H_0$:
\[ T^2 \sim T^2_{m,n-1} \]
\end{enumerate}
\end{ejercicio}
\begin{solucion}
\begin{enumerate}[(a)]
\item[]
\item Vamos a aplicar el apartado b) del apartado anterior. Sabemos que $\media{\mX}-\media{\mu}\sim N_p(0,  \frac{1}{n}\Sigma)$. Por tanto $n^{1/2}(\media{\mX}-\media{\mu})\sim N_p(0, \Sigma)$ y estamos en las condiciones para aplicar el apartado.
\item Bajo la hipótesis $H_0$, tenemos que $n^{1/2}(A\media{\mX}-\muestra{d})\sim N_p(0,A\Sigma A')$ y, usando el Ejerciocio 1.3,  $ASA'\sim W_p(n-1,A\Sigma A')$. Basta aplicar un razonamiento análogo al del apartado anterior.
\end{enumerate}
\end{solucion}
\end{document}
